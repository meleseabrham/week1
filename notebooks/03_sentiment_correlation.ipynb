{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Correlation between News Sentiment and Stock Movement\n",
    "\n",
    "This notebook performs:\n",
    "1. **Sentiment Analysis**: Quantify the tone of news headlines (positive, negative, neutral)\n",
    "2. **Date Alignment**: Normalize timestamps between news and stock datasets\n",
    "3. **Stock Returns Calculation**: Compute daily percentage changes in stock prices\n",
    "4. **Correlation Analysis**: Test correlation between daily news sentiment scores and stock returns\n",
    "\n",
    "## Libraries Used\n",
    "- **TextBlob**: Simple and effective sentiment analysis\n",
    "- **NLTK**: Natural language processing support\n",
    "- **Pandas**: Data manipulation and alignment\n",
    "- **Scipy**: Statistical correlation analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sentiment analysis libraries\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    print(\"✓ TextBlob imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  TextBlob not found. Installing...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"textblob\"])\n",
    "    from textblob import TextBlob\n",
    "    print(\"✓ TextBlob installed and imported\")\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    # Download required NLTK data\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    print(\"✓ NLTK imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  NLTK not found. Installing...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    print(\"✓ NLTK installed and imported\")\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TASK 3: SENTIMENT ANALYSIS AND CORRELATION WITH STOCK MOVEMENTS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"All imports successful! ✓\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare News Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. LOADING NEWS DATA\n",
      "----------------------------------------------------------------------\n",
      "✓ Loaded news data from C:\\project\\kifya\\Week1\\data\\raw\\raw_analyst_ratings.csv\n",
      "✓ Dataset shape: 1,407,328 rows × 6 columns\n",
      "\n",
      "Columns: ['Unnamed: 0', 'headline', 'url', 'publisher', 'date', 'stock']\n",
      "\n",
      "First few rows:\n",
      "   Unnamed: 0                                           headline  \\\n",
      "0           0            Stocks That Hit 52-Week Highs On Friday   \n",
      "1           1         Stocks That Hit 52-Week Highs On Wednesday   \n",
      "2           2                      71 Biggest Movers From Friday   \n",
      "3           3       46 Stocks Moving In Friday's Mid-Day Session   \n",
      "4           4  B of A Securities Maintains Neutral on Agilent...   \n",
      "\n",
      "                                                 url          publisher  \\\n",
      "0  https://www.benzinga.com/news/20/06/16190091/s...  Benzinga Insights   \n",
      "1  https://www.benzinga.com/news/20/06/16170189/s...  Benzinga Insights   \n",
      "2  https://www.benzinga.com/news/20/05/16103463/7...         Lisa Levin   \n",
      "3  https://www.benzinga.com/news/20/05/16095921/4...         Lisa Levin   \n",
      "4  https://www.benzinga.com/news/20/05/16095304/b...         Vick Meyer   \n",
      "\n",
      "                        date stock  \n",
      "0  2020-06-05 10:30:54-04:00     A  \n",
      "1  2020-06-03 10:45:20-04:00     A  \n",
      "2  2020-05-26 04:30:07-04:00     A  \n",
      "3  2020-05-22 12:45:06-04:00     A  \n",
      "4  2020-05-22 11:38:59-04:00     A  \n"
     ]
    }
   ],
   "source": [
    "# Load news data\n",
    "print(\"\\n1. LOADING NEWS DATA\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "RAW_DATA = Path(\"data/raw/raw_analyst_ratings.csv\")\n",
    "POSSIBLE_PATHS = [\n",
    "    Path(\"data/raw/raw_analyst_ratings.csv\"),\n",
    "    Path(\"../data/raw/raw_analyst_ratings.csv\"),\n",
    "]\n",
    "\n",
    "def resolve_data_path(possible_paths) -> Path:\n",
    "    \"\"\"Return the first existing path for the dataset.\"\"\"\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for relative_path in possible_paths:\n",
    "        for base in [cwd, *cwd.parents]:\n",
    "            candidate = base / relative_path\n",
    "            if candidate.exists():\n",
    "                return candidate\n",
    "    return None\n",
    "\n",
    "data_path = resolve_data_path(POSSIBLE_PATHS)\n",
    "\n",
    "if data_path:\n",
    "    try:\n",
    "        news_df = pd.read_csv(\n",
    "            data_path,\n",
    "            encoding_errors=\"replace\",\n",
    "            on_bad_lines=\"skip\",\n",
    "            low_memory=False\n",
    "        )\n",
    "        print(f\"✓ Loaded news data from {data_path}\")\n",
    "        print(f\"✓ Dataset shape: {news_df.shape[0]:,} rows × {news_df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error reading file: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"✗ News data file not found!\")\n",
    "    raise FileNotFoundError(\"Please ensure data/raw/raw_analyst_ratings.csv exists\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nColumns: {list(news_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(news_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. PREPARING NEWS DATA\n",
      "----------------------------------------------------------------------\n",
      "✓ Removed 0 rows with missing data\n",
      "✓ Remaining rows: 1,407,328\n",
      "\n",
      "Date range: 2009-02-14 00:00:00+00:00 to 2020-06-11 21:12:35+00:00\n",
      "Unique stocks: 6204\n",
      "Stocks: ['A', 'AA', 'AAC', 'AADR', 'AAL', 'AAMC', 'AAME', 'AAN', 'AAOI', 'AAON', 'AAP', 'AAPL', 'AAU', 'AAV', 'AAVL', 'AAWW', 'AAXJ', 'AB', 'ABAC', 'ABAX']...\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare news data\n",
    "print(\"\\n2. PREPARING NEWS DATA\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Convert date column to datetime\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], utc=True, format='mixed', errors='coerce')\n",
    "\n",
    "# Fill missing values\n",
    "news_df['headline'] = news_df['headline'].fillna(\"\")\n",
    "news_df['stock'] = news_df['stock'].fillna(\"UNKNOWN\")\n",
    "news_df['publisher'] = news_df['publisher'].fillna(\"Unknown\")\n",
    "\n",
    "# Extract date only (normalize to date, removing time component)\n",
    "news_df['date_only'] = news_df['date'].dt.date\n",
    "\n",
    "# Filter out rows with missing dates or headlines\n",
    "initial_count = len(news_df)\n",
    "news_df = news_df.dropna(subset=['date', 'headline'])\n",
    "news_df = news_df[news_df['headline'].str.strip() != \"\"]\n",
    "\n",
    "print(f\"✓ Removed {initial_count - len(news_df):,} rows with missing data\")\n",
    "print(f\"✓ Remaining rows: {len(news_df):,}\")\n",
    "\n",
    "# Show date range\n",
    "print(f\"\\nDate range: {news_df['date'].min()} to {news_df['date'].max()}\")\n",
    "print(f\"Unique stocks: {news_df['stock'].nunique()}\")\n",
    "print(f\"Stocks: {sorted(news_df['stock'].unique())[:20]}...\")  # Show first 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform Sentiment Analysis on Headlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. PERFORMING SENTIMENT ANALYSIS\n",
      "----------------------------------------------------------------------\n",
      "Analyzing sentiment for all headlines...\n",
      "This may take a few minutes for large datasets...\n",
      "  Processed 100,000 / 1,407,328 headlines...\n",
      "  Processed 200,000 / 1,407,328 headlines...\n",
      "  Processed 300,000 / 1,407,328 headlines...\n",
      "  Processed 400,000 / 1,407,328 headlines...\n",
      "  Processed 500,000 / 1,407,328 headlines...\n",
      "  Processed 600,000 / 1,407,328 headlines...\n",
      "  Processed 700,000 / 1,407,328 headlines...\n",
      "  Processed 800,000 / 1,407,328 headlines...\n",
      "  Processed 900,000 / 1,407,328 headlines...\n"
     ]
    }
   ],
   "source": [
    "# Perform sentiment analysis\n",
    "print(\"\\n3. PERFORMING SENTIMENT ANALYSIS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of a text using TextBlob.\n",
    "    Returns: (polarity, subjectivity)\n",
    "    - Polarity: [-1, 1] where -1 is negative, 1 is positive\n",
    "    - Subjectivity: [0, 1] where 0 is objective, 1 is subjective\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    try:\n",
    "        blob = TextBlob(str(text))\n",
    "        return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "    except Exception as e:\n",
    "        # Return neutral sentiment if analysis fails\n",
    "        return 0.0, 0.0\n",
    "\n",
    "# Apply sentiment analysis (this may take a while for large datasets)\n",
    "print(\"Analyzing sentiment for all headlines...\")\n",
    "print(\"This may take a few minutes for large datasets...\")\n",
    "\n",
    "# Process in batches to show progress\n",
    "batch_size = 10000\n",
    "total_batches = (len(news_df) // batch_size) + 1\n",
    "\n",
    "sentiment_results = []\n",
    "for i in range(0, len(news_df), batch_size):\n",
    "    batch = news_df.iloc[i:i+batch_size]\n",
    "    batch_sentiments = batch['headline'].apply(analyze_sentiment)\n",
    "    sentiment_results.extend(batch_sentiments)\n",
    "    \n",
    "    if (i // batch_size + 1) % 10 == 0 or i + batch_size >= len(news_df):\n",
    "        print(f\"  Processed {min(i+batch_size, len(news_df)):,} / {len(news_df):,} headlines...\")\n",
    "\n",
    "# Extract polarity and subjectivity\n",
    "news_df['sentiment_polarity'] = [s[0] for s in sentiment_results]\n",
    "news_df['sentiment_subjectivity'] = [s[1] for s in sentiment_results]\n",
    "\n",
    "# Classify sentiment\n",
    "news_df['sentiment_label'] = news_df['sentiment_polarity'].apply(\n",
    "    lambda x: 'positive' if x > 0.1 else ('negative' if x < -0.1 else 'neutral')\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Sentiment analysis complete!\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(news_df['sentiment_label'].value_counts())\n",
    "print(f\"\\nSentiment statistics:\")\n",
    "print(news_df['sentiment_polarity'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sentiment label distribution\n",
    "sentiment_counts = news_df['sentiment_label'].value_counts()\n",
    "axes[0].bar(sentiment_counts.index, sentiment_counts.values, color=['red', 'gray', 'green'])\n",
    "axes[0].set_title('Sentiment Label Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment Label')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Sentiment polarity distribution\n",
    "axes[1].hist(news_df['sentiment_polarity'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(news_df['sentiment_polarity'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {news_df[\"sentiment_polarity\"].mean():.3f}')\n",
    "axes[1].axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "axes[1].set_title('Sentiment Polarity Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Polarity Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/figures/sentiment_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved sentiment distribution plot to reports/figures/sentiment_distribution.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Stock Price Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stock price data\n",
    "print(\"\\n4. LOADING STOCK PRICE DATA\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "PRICE_DIR = Path(\"data/processed/prices\")\n",
    "STOCKS_TO_ANALYZE = ['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']\n",
    "\n",
    "stock_data = {}\n",
    "for stock in STOCKS_TO_ANALYZE:\n",
    "    price_file = PRICE_DIR / f\"{stock}.csv\"\n",
    "    if price_file.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(price_file, parse_dates=[\"Date\"])\n",
    "            df = df.sort_values(\"Date\").set_index(\"Date\")\n",
    "            # Ensure numeric columns\n",
    "            numeric_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "            df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "            df = df.dropna(subset=[\"Close\"])\n",
    "            stock_data[stock] = df\n",
    "            print(f\"✓ {stock}: {len(df):,} trading days ({df.index.min().date()} to {df.index.max().date()})\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {stock}: Error loading - {e}\")\n",
    "    else:\n",
    "        print(f\"✗ {stock}: File not found at {price_file}\")\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(stock_data)} stock datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns for each stock\n",
    "print(\"\\n5. CALCULATING DAILY STOCK RETURNS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "stock_returns = {}\n",
    "for stock, df in stock_data.items():\n",
    "    # Calculate daily percentage return\n",
    "    df['daily_return'] = df['Close'].pct_change() * 100  # Convert to percentage\n",
    "    df['daily_return'] = df['daily_return'].fillna(0)  # First day has no return\n",
    "    \n",
    "    # Create a dataframe with date and return\n",
    "    returns_df = pd.DataFrame({\n",
    "        'date': df.index.date,\n",
    "        'daily_return': df['daily_return'],\n",
    "        'close_price': df['Close']\n",
    "    })\n",
    "    returns_df['date'] = pd.to_datetime(returns_df['date'])\n",
    "    \n",
    "    stock_returns[stock] = returns_df\n",
    "    print(f\"✓ {stock}: Mean return = {returns_df['daily_return'].mean():.4f}%, \"\n",
    "          f\"Std = {returns_df['daily_return'].std():.4f}%\")\n",
    "\n",
    "print(f\"\\n✓ Calculated returns for {len(stock_returns)} stocks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Align Dates and Aggregate Daily Sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate daily sentiment scores by stock\n",
    "print(\"\\n6. AGGREGATING DAILY SENTIMENT SCORES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Group by stock and date, then aggregate sentiment\n",
    "daily_sentiment = news_df.groupby(['stock', 'date_only']).agg({\n",
    "    'sentiment_polarity': ['mean', 'std', 'count'],\n",
    "    'sentiment_subjectivity': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "daily_sentiment.columns = ['stock', 'date', 'avg_sentiment', 'sentiment_std', 'article_count', 'avg_subjectivity']\n",
    "daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])\n",
    "\n",
    "# Fill NaN std values (when only one article per day)\n",
    "daily_sentiment['sentiment_std'] = daily_sentiment['sentiment_std'].fillna(0)\n",
    "\n",
    "print(f\"✓ Aggregated sentiment for {len(daily_sentiment):,} stock-date combinations\")\n",
    "print(f\"\\nSample aggregated data:\")\n",
    "print(daily_sentiment.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sentiment with stock returns for each stock\n",
    "print(\"\\n7. ALIGNING DATES AND MERGING DATA\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "correlation_results = []\n",
    "merged_data_all = {}\n",
    "\n",
    "for stock in STOCKS_TO_ANALYZE:\n",
    "    if stock not in stock_returns:\n",
    "        print(f\"⚠️  Skipping {stock}: No price data available\")\n",
    "        continue\n",
    "    \n",
    "    # Get stock returns\n",
    "    returns_df = stock_returns[stock].copy()\n",
    "    \n",
    "    # Get sentiment for this stock (handle different stock symbol formats)\n",
    "    # Try exact match first, then try variations\n",
    "    stock_sentiment = daily_sentiment[daily_sentiment['stock'] == stock].copy()\n",
    "    \n",
    "    # If no exact match, try common variations\n",
    "    if len(stock_sentiment) == 0:\n",
    "        # Try with 'L' suffix (e.g., GOOGL -> GOOG)\n",
    "        if stock == 'GOOG':\n",
    "            stock_sentiment = daily_sentiment[daily_sentiment['stock'] == 'GOOGL'].copy()\n",
    "        # Try other variations if needed\n",
    "    \n",
    "    if len(stock_sentiment) == 0:\n",
    "        print(f\"⚠️  {stock}: No sentiment data found\")\n",
    "        continue\n",
    "    \n",
    "    # Merge on date\n",
    "    merged = pd.merge(\n",
    "        returns_df,\n",
    "        stock_sentiment[['date', 'avg_sentiment', 'article_count', 'avg_subjectivity']],\n",
    "        on='date',\n",
    "        how='inner'  # Only keep dates with both sentiment and returns\n",
    "    )\n",
    "    \n",
    "    if len(merged) == 0:\n",
    "        print(f\"⚠️  {stock}: No overlapping dates between sentiment and returns\")\n",
    "        continue\n",
    "    \n",
    "    merged_data_all[stock] = merged\n",
    "    \n",
    "    # Calculate correlation\n",
    "    if len(merged) > 1:  # Need at least 2 data points for correlation\n",
    "        correlation, p_value = pearsonr(merged['avg_sentiment'], merged['daily_return'])\n",
    "        correlation_results.append({\n",
    "            'stock': stock,\n",
    "            'correlation': correlation,\n",
    "            'p_value': p_value,\n",
    "            'n_observations': len(merged),\n",
    "            'date_range': f\"{merged['date'].min().date()} to {merged['date'].max().date()}\"\n",
    "        })\n",
    "        print(f\"✓ {stock}: Correlation = {correlation:.4f} (p={p_value:.4f}), \"\n",
    "              f\"N={len(merged):,} days\")\n",
    "    else:\n",
    "        print(f\"⚠️  {stock}: Insufficient data for correlation\")\n",
    "\n",
    "print(f\"\\n✓ Merged data for {len(merged_data_all)} stocks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display correlation results\n",
    "print(\"\\n8. CORRELATION ANALYSIS RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if correlation_results:\n",
    "    results_df = pd.DataFrame(correlation_results)\n",
    "    results_df = results_df.sort_values('correlation', ascending=False)\n",
    "    \n",
    "    print(\"\\nCorrelation between Daily Sentiment and Stock Returns:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(\"data/processed/sentiment_correlation\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results_df.to_csv(output_dir / \"correlation_results.csv\", index=False)\n",
    "    print(f\"\\n✓ Saved correlation results to {output_dir / 'correlation_results.csv'}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    print(f\"  Mean correlation: {results_df['correlation'].mean():.4f}\")\n",
    "    print(f\"  Median correlation: {results_df['correlation'].median():.4f}\")\n",
    "    print(f\"  Std correlation: {results_df['correlation'].std():.4f}\")\n",
    "    print(f\"  Significant correlations (p < 0.05): {(results_df['p_value'] < 0.05).sum()}\")\n",
    "    print(f\"  Strong correlations (|r| > 0.3): {(results_df['correlation'].abs() > 0.3).sum()}\")\n",
    "else:\n",
    "    print(\"⚠️  No correlation results available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation results\n",
    "if correlation_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    results_df = pd.DataFrame(correlation_results)\n",
    "    \n",
    "    # 1. Bar plot of correlations\n",
    "    axes[0, 0].barh(results_df['stock'], results_df['correlation'], \n",
    "                    color=['green' if x > 0 else 'red' for x in results_df['correlation']])\n",
    "    axes[0, 0].axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "    axes[0, 0].set_xlabel('Correlation Coefficient')\n",
    "    axes[0, 0].set_title('Sentiment-Return Correlation by Stock', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 2. P-value visualization\n",
    "    axes[0, 1].barh(results_df['stock'], -np.log10(results_df['p_value'] + 1e-10),\n",
    "                    color='steelblue')\n",
    "    axes[0, 1].axvline(-np.log10(0.05), color='red', linestyle='--', \n",
    "                       label='p=0.05 threshold')\n",
    "    axes[0, 1].set_xlabel('-log10(p-value)')\n",
    "    axes[0, 1].set_title('Statistical Significance', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 3. Scatter plot: Correlation vs Sample Size\n",
    "    scatter = axes[1, 0].scatter(results_df['n_observations'], results_df['correlation'],\n",
    "                                s=100, c=results_df['p_value'], cmap='RdYlGn_r',\n",
    "                                edgecolors='black', linewidth=1)\n",
    "    axes[1, 0].axhline(0, color='black', linestyle='-', linewidth=1, alpha=0.3)\n",
    "    axes[1, 0].set_xlabel('Number of Observations')\n",
    "    axes[1, 0].set_ylabel('Correlation Coefficient')\n",
    "    axes[1, 0].set_title('Correlation vs Sample Size', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[1, 0], label='p-value')\n",
    "    \n",
    "    # Add stock labels\n",
    "    for idx, row in results_df.iterrows():\n",
    "        axes[1, 0].annotate(row['stock'], \n",
    "                           (row['n_observations'], row['correlation']),\n",
    "                           fontsize=8, alpha=0.7)\n",
    "    \n",
    "    # 4. Distribution of correlations\n",
    "    axes[1, 1].hist(results_df['correlation'], bins=15, color='steelblue', \n",
    "                    edgecolor='black', alpha=0.7)\n",
    "    axes[1, 1].axvline(results_df['correlation'].mean(), color='red', \n",
    "                       linestyle='--', label=f'Mean: {results_df[\"correlation\"].mean():.3f}')\n",
    "    axes[1, 1].axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "    axes[1, 1].set_xlabel('Correlation Coefficient')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Distribution of Correlations', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reports/figures/sentiment_correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved correlation analysis plot to reports/figures/sentiment_correlation_analysis.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis: Sentiment vs Returns for Individual Stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed scatter plots for each stock\n",
    "print(\"\\n9. CREATING DETAILED VISUALIZATIONS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "n_stocks = len(merged_data_all)\n",
    "if n_stocks > 0:\n",
    "    cols = 3\n",
    "    rows = (n_stocks + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(18, 6*rows))\n",
    "    axes = axes.flatten() if n_stocks > 1 else [axes]\n",
    "    \n",
    "    for idx, (stock, merged) in enumerate(merged_data_all.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Scatter plot\n",
    "        scatter = ax.scatter(merged['avg_sentiment'], merged['daily_return'],\n",
    "                           alpha=0.6, s=50, c=merged['article_count'],\n",
    "                           cmap='viridis', edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(merged['avg_sentiment'], merged['daily_return'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax.plot(merged['avg_sentiment'], p(merged['avg_sentiment']), \n",
    "               \"r--\", alpha=0.8, linewidth=2, label=f'Trend line')\n",
    "        \n",
    "        # Calculate and display correlation\n",
    "        corr, p_val = pearsonr(merged['avg_sentiment'], merged['daily_return'])\n",
    "        ax.set_title(f'{stock}: r={corr:.3f}, p={p_val:.4f}\\n'\n",
    "                    f'N={len(merged):,} days', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Average Daily Sentiment')\n",
    "        ax.set_ylabel('Daily Return (%)')\n",
    "        ax.axhline(0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "        ax.axvline(0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(scatter, ax=ax, label='Article Count')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_stocks, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reports/figures/sentiment_vs_returns_by_stock.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved detailed scatter plots to reports/figures/sentiment_vs_returns_by_stock.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time Series Analysis: Sentiment and Returns Over Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series visualization for top correlated stocks\n",
    "if correlation_results:\n",
    "    results_df = pd.DataFrame(correlation_results)\n",
    "    # Get top 3 stocks by absolute correlation\n",
    "    top_stocks = results_df.nlargest(3, 'correlation')['stock'].tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(len(top_stocks), 1, figsize=(16, 5*len(top_stocks)))\n",
    "    if len(top_stocks) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, stock in enumerate(top_stocks):\n",
    "        if stock in merged_data_all:\n",
    "            merged = merged_data_all[stock].sort_values('date')\n",
    "            \n",
    "            ax1 = axes[idx]\n",
    "            ax2 = ax1.twinx()\n",
    "            \n",
    "            # Plot sentiment\n",
    "            line1 = ax1.plot(merged['date'], merged['avg_sentiment'], \n",
    "                            color='blue', alpha=0.7, linewidth=1.5, label='Sentiment')\n",
    "            ax1.axhline(0, color='black', linestyle='--', linewidth=0.5, alpha=0.3)\n",
    "            ax1.set_ylabel('Average Sentiment', color='blue', fontsize=12)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "            ax1.grid(alpha=0.3)\n",
    "            \n",
    "            # Plot returns\n",
    "            line2 = ax2.plot(merged['date'], merged['daily_return'], \n",
    "                            color='red', alpha=0.7, linewidth=1.5, label='Daily Return')\n",
    "            ax2.axhline(0, color='black', linestyle='--', linewidth=0.5, alpha=0.3)\n",
    "            ax2.set_ylabel('Daily Return (%)', color='red', fontsize=12)\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            \n",
    "            # Combine legends\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax1.legend(lines, labels, loc='upper left')\n",
    "            \n",
    "            corr, p_val = pearsonr(merged['avg_sentiment'], merged['daily_return'])\n",
    "            ax1.set_title(f'{stock}: Sentiment vs Returns Over Time (r={corr:.3f}, p={p_val:.4f})',\n",
    "                         fontsize=14, fontweight='bold')\n",
    "            ax1.set_xlabel('Date')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reports/figures/sentiment_returns_timeseries.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved time series plots to reports/figures/sentiment_returns_timeseries.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "print(\"\\n10. SAVING PROCESSED DATA\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "output_dir = Path(\"data/processed/sentiment_correlation\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save daily sentiment aggregated data\n",
    "daily_sentiment.to_csv(output_dir / \"daily_sentiment_by_stock.csv\", index=False)\n",
    "print(f\"✓ Saved daily sentiment data: {len(daily_sentiment):,} rows\")\n",
    "\n",
    "# Save merged data for each stock\n",
    "for stock, merged in merged_data_all.items():\n",
    "    merged.to_csv(output_dir / f\"{stock}_sentiment_returns.csv\", index=False)\n",
    "    print(f\"✓ Saved {stock} merged data: {len(merged):,} rows\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'total_articles_analyzed': len(news_df),\n",
    "    'unique_stocks_in_news': news_df['stock'].nunique(),\n",
    "    'date_range_news': f\"{news_df['date'].min()} to {news_df['date'].max()}\",\n",
    "    'stocks_analyzed': list(merged_data_all.keys()),\n",
    "    'correlation_summary': {\n",
    "        'mean': results_df['correlation'].mean() if correlation_results else None,\n",
    "        'median': results_df['correlation'].median() if correlation_results else None,\n",
    "        'std': results_df['correlation'].std() if correlation_results else None,\n",
    "        'significant_count': (results_df['p_value'] < 0.05).sum() if correlation_results else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(output_dir / \"analysis_summary.json\", 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2, default=str)\n",
    "print(f\"✓ Saved analysis summary\")\n",
    "\n",
    "print(f\"\\n✓ All data saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Findings and Insights\n",
    "\n",
    "### Summary of Analysis:\n",
    "\n",
    "1. **Sentiment Analysis**: Used TextBlob to analyze sentiment polarity for all news headlines\n",
    "2. **Date Alignment**: Normalized timestamps and aligned news dates with stock trading days\n",
    "3. **Daily Aggregation**: Computed average daily sentiment scores when multiple articles appeared on the same day\n",
    "4. **Stock Returns**: Calculated daily percentage returns from closing prices\n",
    "5. **Correlation Analysis**: Computed Pearson correlation coefficients between sentiment and returns\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **Correlation Coefficient (r)**: \n",
    "  - Range: -1 to +1\n",
    "  - Positive values indicate positive sentiment is associated with positive returns\n",
    "  - Negative values indicate negative sentiment is associated with positive returns (contrarian effect)\n",
    "  - Values close to 0 indicate weak or no relationship\n",
    "\n",
    "- **P-value**: \n",
    "  - p < 0.05 indicates statistically significant correlation\n",
    "  - Lower p-values indicate stronger evidence of a relationship\n",
    "\n",
    "### References:\n",
    "\n",
    "1. **TextBlob**: Simple, Pythonic text processing library\n",
    "   - Documentation: https://textblob.readthedocs.io/\n",
    "   - Sentiment analysis based on pattern library\n",
    "\n",
    "2. **Pearson Correlation**: Standard measure of linear relationship\n",
    "   - Scipy documentation: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\n",
    "\n",
    "3. **Financial Sentiment Analysis**:\n",
    "   - Loughran, T., & McDonald, B. (2011). When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks. The Journal of Finance, 66(1), 35-65.\n",
    "   - Bollen, J., Mao, H., & Zeng, X. (2011). Twitter mood predicts the stock market. Journal of Computational Science, 2(1), 1-8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
